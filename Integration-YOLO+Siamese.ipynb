{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada42c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import imutils\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from scipy.spatial import distance\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from skimage import io\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1af2073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are now using yolov4-tiny-custom_final.weights weights ,yolov4-tiny-custom.cfg configs and labels.txt labels.\n"
     ]
    }
   ],
   "source": [
    "#model config paths\n",
    "weights = glob.glob(\"yolov4-tiny-custom_final.weights\")[0]\n",
    "labels = glob.glob(\"labels.txt\")[0]\n",
    "cfg = glob.glob(\"yolov4-tiny-custom.cfg\")[0]\n",
    "print(\"You are now using {} weights ,{} configs and {} labels.\".format(weights, cfg, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b6da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "lbls = []\n",
    "with open(labels, \"r\") as f:\n",
    "    lbls = [c.strip() for c in f.readlines()]\n",
    "\n",
    "COLORS = np.random.randint(0, 255, size=(len(lbls), 3), dtype=\"uint8\")\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(cfg, weights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "layer = net.getLayerNames()\n",
    "layer = [layer[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2354df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD= 0.5\n",
    "NMS_THRESHOLD= 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786ab3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image, nn):\n",
    "    #image = cv2.imread(imgpath)\n",
    "    #image = cv2.imdecode(np.fromfile(imgpath, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(image, 1/255, (416, 416), swapRB=True, crop=False)\n",
    "    nn.setInput(blob)\n",
    "    start_time = time.time()\n",
    "    layer_outs = nn.forward(layer)\n",
    "    end_time = time.time()\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for output in layer_outs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > CONFIDENCE_THRESHOLD:\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (center_x, center_y, width, height) = box.astype(\"int\")\n",
    "\n",
    "                x = int(center_x - (width / 2))\n",
    "                y = int(center_y - (height / 2))\n",
    "\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    bbcoordinates=[]\n",
    "    classnum=[]\n",
    "    \n",
    "    if len(idxs) > 0:\n",
    "        for i in idxs.flatten():\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            color = [int(c) for c in COLORS[class_ids[i]]]\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), color, 2)\n",
    "            text = \"{}: {:.4f}\".format(lbls[class_ids[i]], confidences[i])\n",
    "#             cv2.putText(image, text, (x, y -5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            label = \"Inference Time: {:.2f} ms\".format(end_time - start_time)\n",
    "#             cv2.putText(image, label, (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "            bbcoordinates.append((x,y,w,h))\n",
    "            classnum.append(class_ids[i])\n",
    "        #print(bbcoordinates)\n",
    "#         print(class_ids[i])\n",
    "        cv2.imshow(\"image\", image)\n",
    "        cv2.imwrite(\"output.png\", image)\n",
    "        cv2.waitKey(0)\n",
    "        return image, bbcoordinates, classnum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285aac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH=416\n",
    "IMG_HEIGHT=416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67e329a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread('test.jpg')\n",
    "# img = np.asarray(bytearray(blob.download_as_bytes()), dtype=np.uint8)\n",
    "# img = cv2.imdecode(img, flags=1)\n",
    "img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d1399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,bbox,id=detect(img,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab4490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_objects(image, bbox, classname):\n",
    "    counts=dict()\n",
    "    for i in range(len(bbox)):\n",
    "        ing_name= classname[i]\n",
    "        counts[ing_name] = counts.get(ing_name, 0) + 1\n",
    "        x,y,w,h= bbox[i]\n",
    "        #print(x,y,w,h)\n",
    "        abs_x=abs(x)\n",
    "        abs_y=abs(y)\n",
    "        abs_w=abs(w)\n",
    "        abs_h=abs(h)\n",
    "        print(abs_x, abs_y,abs_w, abs_h)\n",
    "        cropped_img = image[abs_y: abs_y+abs_h, abs_x: abs_x+abs_w]\n",
    "        # construct image name and join it to path for saving crop properly\n",
    "        #img_name = ing_name + '_' + str(counts[ing_name]) + '.jpg'\n",
    "        img_name= str(i)+'.jpg'\n",
    "        \n",
    "        if not os.path.isdir(\"cropped_images/demo_folder\"):\n",
    "            os.makedirs(\"cropped_images/demo_folder\")\n",
    "        \n",
    "        path=r\"cropped_images/demo_folder\"\n",
    "        img_path = os.path.join(path, img_name )\n",
    "        # save image\n",
    "        cv2.imwrite(img_path, cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb7a42a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 190 102 50\n",
      "107 108 93 54\n",
      "284 278 91 49\n",
      "193 56 54 38\n",
      "317 232 87 43\n"
     ]
    }
   ],
   "source": [
    "crop_objects(image,bbox, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d27a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vconcat_resize_min(im_list, interpolation=cv2.INTER_CUBIC):\n",
    "    w_min = min(im.shape[1] for im in im_list)\n",
    "    im_list_resize = [cv2.resize(im, (w_min, int(im.shape[0] * w_min / im.shape[1])), interpolation=interpolation)\n",
    "                      for im in im_list]\n",
    "    return cv2.vconcat(im_list_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c186ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hconcat_resize_min(im_list, interpolation=cv2.INTER_CUBIC):\n",
    "    h_min = min(im.shape[0] for im in im_list)\n",
    "    im_list_resize = [cv2.resize(im, (int(im.shape[1] * h_min / im.shape[0]), h_min), interpolation=interpolation)\n",
    "                      for im in im_list]\n",
    "    return cv2.hconcat(im_list_resize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5159343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tile_resize(img_list_2d, interpolation=cv2.INTER_CUBIC):\n",
    "    img_list_v=[hconcat_resize_min(img_list_h, interpolation=cv2.INTER_CUBIC) for img_list_h in img_list_2d]\n",
    "    return vconcat_resize_min(img_list_v, interpolation=cv2.INTER_CUBIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "180d33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(img):\n",
    "    img2=img\n",
    "    img3=img\n",
    "    height=img.shape[0]\n",
    "    width=img.shape[1]\n",
    "    if height>width and (width/height)<0.5:\n",
    "        image = hconcat_resize_min([img, img2, img3])\n",
    "    elif height<width and (height/width)<0.5:\n",
    "        image = vconcat_resize_min([img, img2, img3])\n",
    "    elif height<=100 and width<=100:\n",
    "        image= concat_tile_resize([[img,img2,img3],[img,img2,img3],[img,img2,img3]])\n",
    "    else:\n",
    "        image=img\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd30d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 198, 198, 16)      448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 196, 196, 32)      4640      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 196, 196, 64)      2112      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 98, 98, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 96, 96, 16)        9232      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 94, 94, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 94, 94, 64)        2112      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 47, 47, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 45, 45, 16)        9232      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 43, 43, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 43, 43, 64)        2112      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 21, 21, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 10, 10, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 8, 8, 16)          9232      \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 6, 6, 32)          4640      \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 6, 6, 64)          2112      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 3, 3, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               73856     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145,520\n",
      "Trainable params: 145,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('sia_model_new.h5', compile=False)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388481cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder containing the images\n",
    "folder_path = 'cropped_images/demo_folder'\n",
    "\n",
    "# create an empty list to store the images\n",
    "images = []\n",
    "\n",
    "# loop through all the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # only read image files\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # read the image\n",
    "        img = cv2.imread(os.path.join(folder_path, filename))\n",
    "        # add the image to the list\n",
    "        images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc9d2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 200\n",
    "h = 200\n",
    "columns = 3\n",
    "rows = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30cf7e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "0.048391122\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "0.078751296\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "0.025918338\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "0.06726191\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "0.017228803\n"
     ]
    }
   ],
   "source": [
    "duplicates_img_list=[]\n",
    "duplicates_bbox_list=[]\n",
    "duplicates_indices=[]\n",
    "i=0\n",
    "\n",
    "while i<len(images):\n",
    "    anchor= images[i]\n",
    "    anchor= preprocessing(anchor)\n",
    "    anchor= cv2.resize(anchor,(w,h))\n",
    "    anchor_bbox= bbox[i]\n",
    "    \n",
    "    for j in range(i+1,len(images)):\n",
    "        positive= images[j]\n",
    "        positive= preprocessing(positive)\n",
    "        positive= cv2.resize(positive,(w,h))\n",
    "        positive_bbox= bbox[j]\n",
    "        negative= images[j]\n",
    "        negative= preprocessing(negative)\n",
    "        negative= cv2.resize(negative,(w,h))\n",
    "        \n",
    "        image_set= np.stack([anchor/ 255. , positive/ 255. , negative/ 255. ], axis=0)\n",
    "        output = new_model.predict(image_set)\n",
    "        \n",
    "        post_dst= 0.5*(tf.math.reduce_variance(tf.subtract(output[0,:], output[1,:]))   / \\\n",
    "                    (tf.math.reduce_variance(output[0,:])+tf.math.reduce_variance(output[1,:])))\n",
    "        print(post_dst.numpy())\n",
    "        \n",
    "        cv2.imshow('anc',anchor)\n",
    "        cv2.imshow('pos',positive)\n",
    "        cv2.waitKey(0)\n",
    "        \n",
    "        if post_dst <0.04:\n",
    "            duplicates_img_list.append(positive)\n",
    "            duplicates_bbox_list.append(positive_bbox)\n",
    "            duplicates_indices.append((i,j))\n",
    "            images.pop(j)\n",
    "            bbox.pop(j)\n",
    "            break\n",
    "    i += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b242fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "print(len(images), len(bbox))\n",
    "print(len(duplicates_img_list), len(duplicates_bbox_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d353248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in images:\n",
    "    cv2.imshow('a',i)\n",
    "    cv2.waitKey(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in duplicates_img_list:\n",
    "    cv2.imshow('d', i)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23627605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(88, 190, 102, 50), (107, 108, 93, 54), (284, 278, 91, 49)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "539ba200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(193, 56, 54, 38), (317, 232, 87, 43)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates_bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e014471b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3), (1, 3)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbb1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
